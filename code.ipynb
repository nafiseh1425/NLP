{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport transformers as ppb # pytorch transformers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n# loading and reading data\n\ndf = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\ndf.head()\n\n#Importing pre-trained DistilBERT model and tokenizer\n\nmodel_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n\n## Want BERT instead of distilBERT? Uncomment the following line:\n#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n\n# Load pretrained model/tokenizer\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)\n\n\n#TOkenize all the sentences\ntokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\ntokenized\n\n#Padding\nmax_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n\nnp.array(padded).shape\n\n\n#masking\nattention_mask = np.where(padded != 0, 1, 0)\nattention_mask.shape\n\n#Processing with DistilBERT:\ninput_ids = torch.tensor(padded)  \nattention_mask = torch.tensor(attention_mask)\n\nwith torch.no_grad():\n    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\nfeatures = last_hidden_states[0][:,0,:].numpy()\n\n\n#Train/Test Split:\nlabels = df[1]\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n\nlr_clf = LogisticRegression()\nlr_clf.fit(train_features, train_labels)\n\nlr_clf.score(test_features, test_labels)\n#Grid Search for Parameters\n\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'C': np.linspace(0.0001, 100, 20)}\ngrid_search = GridSearchCV(LogisticRegression(), parameters)\ngrid_search.fit(train_features, train_labels)\n\nprint('best parameters: ', grid_search.best_params_)\nprint('best scrores: ', grid_search.best_score_)\nCb = grid_search.best_params_['C']","metadata":{},"execution_count":null,"outputs":[]}]}
